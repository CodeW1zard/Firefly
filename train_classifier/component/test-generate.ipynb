{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca9e2d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-01 15:42:28.427\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1mGenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"max_new_tokens\": 4,\n",
      "  \"temperature\": 0.35,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AddedToken\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "import torch\n",
    "from loguru import logger\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "from utils import ModelUtils\n",
    "from template import template_dict\n",
    "\n",
    "\n",
    "def build_prompt_chatglm3(tokenizer, query, history, system=None):\n",
    "    history.append({\"role\": 'user', 'message': query})\n",
    "    # system\n",
    "    input_ids = tokenizer.get_prefix_tokens() + \\\n",
    "                [tokenizer.get_command(f\"<|system|>\")] + \\\n",
    "                tokenizer.encode(system, add_special_tokens=False)\n",
    "    # convs\n",
    "    for item in history:\n",
    "        role, message = item['role'], item['message']\n",
    "        if role == 'user':\n",
    "            tokens = [tokenizer.get_command(f\"<|user|>\")] + \\\n",
    "                     tokenizer.encode(message, add_special_tokens=False) + \\\n",
    "                     [tokenizer.get_command(f\"<|assistant|>\")]\n",
    "        else:\n",
    "            tokens = tokenizer.encode(message, add_special_tokens=False) + [tokenizer.eos_token_id]\n",
    "        input_ids += tokens\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def build_prompt(tokenizer, template, query, history, system=None):\n",
    "    template_name = template.template_name\n",
    "    system_format = template.system_format\n",
    "    user_format = template.user_format\n",
    "    assistant_format = template.assistant_format\n",
    "    system = system if system is not None else template.system\n",
    "\n",
    "    if template_name == 'chatglm2':\n",
    "        prompt = tokenizer.build_prompt(query, history)\n",
    "        input_ids = tokenizer.encode(prompt)\n",
    "    elif template_name == 'chatglm3':\n",
    "        input_ids = build_prompt_chatglm3(tokenizer, query, history, system)\n",
    "    else:\n",
    "        history.append({\"role\": 'user', 'message': query})\n",
    "        input_ids = []\n",
    "\n",
    "        # setting system information\n",
    "        if system_format is not None:\n",
    "            # system信息不为空\n",
    "            if system is not None:\n",
    "                system_text = system_format.format(content=system)\n",
    "                input_ids = tokenizer.encode(system_text, add_special_tokens=False)\n",
    "        # concat conversation\n",
    "        for item in history:\n",
    "            role, message = item['role'], item['message']\n",
    "            if role == 'user':\n",
    "                message = user_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "            else:\n",
    "                message = assistant_format.format(content=message, stop_token=tokenizer.eos_token)\n",
    "            tokens = tokenizer.encode(message, add_special_tokens=False)\n",
    "            input_ids += tokens\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def load_tokenizer(model_name_or_path):\n",
    "    # config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "    # 加载tokenzier\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=False\n",
    "        # llama不支持fast\n",
    "        # use_fast=False if config.model_type == 'llama' else True\n",
    "    )\n",
    "\n",
    "    if tokenizer.__class__.__name__ == 'QWenTokenizer':\n",
    "        tokenizer.pad_token_id = tokenizer.eod_id\n",
    "        tokenizer.bos_token_id = tokenizer.eod_id\n",
    "        tokenizer.eos_token_id = tokenizer.eod_id\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # assert tokenizer.pad_token_id is not None, \"pad_token_id should not be None\"\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "\n",
    "model_name_or_path = '/DATA/jupyter/share/LLM_NBS/Baichuan2-13B-Chat'\n",
    "template_name = 'baichuan2'\n",
    "file = './data/v1_prompt_label/test.json'\n",
    "file = '/DATA/jupyter/personal/ChatGLM3/finetune_demo/data/v1_prompt_label/test.json'\n",
    "generation_config = GenerationConfig.from_pretrained('/DATA/jupyter/personal/Firefly/output/')\n",
    "logger.info(generation_config)\n",
    "template = template_dict[template_name]\n",
    "# 是否使用4bit进行推理，能够节省很多显存，但效果可能会有一定的下降\n",
    "load_in_4bit = True\n",
    "# for step in range(200, 901, 100):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     adapter_name_or_path = './output/baichuan2_13b_b1_acc16_epoch3_rk64_a16_lr2e4/checkpoint-%d' % step\n",
    "#     save_file = './data/v1_prompt_label/13b-checkpoint-%d.json' % step\n",
    "#     # 加载模型\n",
    "#     logger.info(f'Loading model from: {model_name_or_path}')\n",
    "#     logger.info(f'adapter_name_or_path: {adapter_name_or_path}')\n",
    "# #         try:\n",
    "#     model = ModelUtils.load_model(\n",
    "#         model_name_or_path,\n",
    "#         load_in_4bit=load_in_4bit,\n",
    "#         adapter_name_or_path=adapter_name_or_path\n",
    "#     ).eval()\n",
    "#     tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)\n",
    "#     wf = open(save_file, 'w')\n",
    "#     st = time.time()\n",
    "#     with open(file, 'r') as rf:\n",
    "#         for cnt, line in enumerate(rf):\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             sample = json.loads(line.strip('\\n'))\n",
    "#             prompt = [sample['conversations'][0]]\n",
    "#             logger.info(f'{cnt} {prompt}')\n",
    "#             response = model.chat(tokenizer, prompt, generation_config=generation_config)\n",
    "#             sample['ans'] = response\n",
    "#             wf.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "#     wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b1a9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0a0+4136153 with CUDA 1201 (you have 2.0.0+cu117)\n",
      "    Python  3.10.12 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b240d7e1258402fa34c8cc889f0f4c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapter_name_or_path = '/DATA/jupyter/personal/Firefly/output/baichuan2_13b_b1_acc16_epoch3_rk64_a16_lr2e4/checkpoint-%d' % 200\n",
    "model = ModelUtils.load_model(\n",
    "    model_name_or_path,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    adapter_name_or_path=adapter_name_or_path\n",
    ").eval()\n",
    "tokenizer = load_tokenizer(model_name_or_path if adapter_name_or_path is None else adapter_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4abf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wf = open(save_file, 'w')\n",
    "# st = time.time()\n",
    "with open(file, 'r') as rf:\n",
    "    for cnt, line in enumerate(rf):\n",
    "        if not line:\n",
    "            continue\n",
    "        sample = json.loads(line.strip('\\n'))\n",
    "        prompt = [sample['conversations'][0]]\n",
    "        break\n",
    "#         logger.info(f'{cnt} {prompt}')\n",
    "#         response = model.chat(tokenizer, prompt, generation_config=generation_config)\n",
    "#         sample['ans'] = response\n",
    "#         wf.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "# wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266a796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = template_dict[template_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96ab710",
   "metadata": {},
   "outputs": [],
   "source": [
    "if template.stop_word is None:\n",
    "    template.stop_word = tokenizer.eos_token\n",
    "stop_token_id = tokenizer.encode(template.stop_word, add_special_tokens=False)\n",
    "assert len(stop_token_id) == 1\n",
    "stop_token_id = stop_token_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd97415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = build_prompt(tokenizer, template, prompt[0]['content'], [], system=None).to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb7b355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f0343b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"do_sample\": true,\n",
       "  \"max_new_tokens\": 4,\n",
       "  \"temperature\": 0.35,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c458cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.35 s, sys: 37.5 s, total: 43.9 s\n",
      "Wall time: 5.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbeb50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.21 ms, sys: 0 ns, total: 1.21 ms\n",
      "Wall time: 920 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'判断为0'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "outputs = outputs.tolist()[0][len(input_ids[0]):]\n",
    "response = tokenizer.decode(outputs)\n",
    "response = response.strip().replace(template.stop_word, \"\").strip()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f031b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31f4a32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22813.427734375\n",
      "24468.0\n"
     ]
    }
   ],
   "source": [
    "memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28149f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a02c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'判断为0</s'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c676cbc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbe6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = []\n",
    "\n",
    "query = input('User：')\n",
    "while True:\n",
    "    query = query.strip()\n",
    "    input_ids = build_prompt(tokenizer, template, query, copy.deepcopy(history), system=None).to(model.device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids, max_new_tokens=max_new_tokens, do_sample=True,\n",
    "        top_p=top_p, temperature=temperature, repetition_penalty=repetition_penalty,\n",
    "        eos_token_id=stop_token_id\n",
    "    )\n",
    "    outputs = outputs.tolist()[0][len(input_ids[0]):]\n",
    "    response = tokenizer.decode(outputs)\n",
    "    response = response.strip().replace(template.stop_word, \"\").strip()\n",
    "    # update history\n",
    "    history.append({\"role\": 'user', 'message': query})\n",
    "    history.append({\"role\": 'assistant', 'message': response})\n",
    "\n",
    "    print(\"Firefly：{}\".format(response))\n",
    "    query = input('User：')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
