{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "702ec588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0a0+4136153 with CUDA 1201 (you have 2.0.0+cu117)\n",
      "    Python  3.10.12 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-01 17:53:12,096] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from loguru import logger\n",
    "from modeling_classifier import BaichuanForSequenceClassification\n",
    "import bitsandbytes as bnb\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import DPOTrainer, get_kbit_device_map\n",
    "import torch \n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "def load_tokenizer(args):\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path, trust_remote_code=True)\n",
    "    # 加载tokenzier\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        # llama不支持fast\n",
    "        use_fast=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    assert tokenizer.pad_token_id is not None, \"pad_token_id should not be None\"\n",
    "    assert tokenizer.eos_token_id is not None, \"eos_token_id should not be None\"\n",
    "    logger.info(f'vocab_size of tokenizer: {tokenizer.vocab_size}')\n",
    "    return tokenizer\n",
    "\n",
    "def load_model(args):\n",
    "    \"\"\"\n",
    "    加载模型\n",
    "    \"\"\"\n",
    "    logger.info(f'Loading model from base model: {args.model_name_or_path}')\n",
    "    logger.info(f'Train model with {args.train_mode}')\n",
    "\n",
    "    # init model kwargs\n",
    "    # todo add flash attention\n",
    "    # attn_implementation = None\n",
    "    torch_dtype = torch.float16 if args.fp16 else torch.float32\n",
    "    if args.train_mode == 'qlora':\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16, # if training_args.args.fp16 else torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "    else:\n",
    "        quantization_config = None\n",
    "    logger.info(quantization_config)\n",
    "    model_kwargs = dict(\n",
    "        trust_remote_code=True,\n",
    "        # attn_implementation=attn_implementation,\n",
    "        torch_dtype=torch_dtype,\n",
    "        use_cache=False if args.gradient_checkpointing else True,\n",
    "        device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    model = BaichuanForSequenceClassification.from_pretrained(args.model_name_or_path, **model_kwargs)\n",
    "\n",
    "    # moe模型，需要考虑负载均衡的loss\n",
    "    if 'output_router_logits' in model.config.to_dict():\n",
    "        logger.info('set output_router_logits as True')\n",
    "        model.config.output_router_logits = True\n",
    "    # QLoRA: casts all the non int8 modules to full precision (fp32) for stability\n",
    "    if args.train_mode == 'qlora':\n",
    "        model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=args.gradient_checkpointing)\n",
    "    # LoRA: Enables the gradients for the input embeddings\n",
    "    if args.train_mode == 'lora':\n",
    "        # For backward compatibility\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "        else:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    # init peft_config\n",
    "#     peft_config = None\n",
    "    if args.train_mode == 'full':\n",
    "        peft_config = None\n",
    "    else:\n",
    "        # 找到所有需要插入adapter的全连接层\n",
    "        #target_modules = find_all_linear_names(model, args.train_mode)\n",
    "        target_modules = ['W_pack']\n",
    "        peft_config = LoraConfig(\n",
    "            r=args.lora_rank,\n",
    "            lora_alpha=args.lora_alpha,\n",
    "            target_modules=target_modules,\n",
    "            lora_dropout=args.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=args.task_type,\n",
    "        )\n",
    "\n",
    "    # init peft model\n",
    "    if args.train_mode in ['lora', 'qlora']:\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "        logger.info(f'memory footprint of model: {model.get_memory_footprint() / (1024 * 1024 * 1024)} GB')\n",
    "\n",
    "\n",
    "    # 计算模型参数量\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    logger.info(\"Total model params: %.2fM\" % (total / 1e6))\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'peft_config': peft_config\n",
    "    }\n",
    "\n",
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_cached()/1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4866eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-01 17:53:15.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_tokenizer\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mvocab_size of tokenizer: 125696\u001b[0m\n",
      "\u001b[32m2024-03-01 17:53:15.953\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mLoading model from base model: /DATA/jupyter/share/LLM_NBS/Baichuan2-7B-Chat/\u001b[0m\n",
      "\u001b[32m2024-03-01 17:53:15.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mTrain model with qlora\u001b[0m\n",
      "\u001b[32m2024-03-01 17:53:15.956\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m49\u001b[0m - \u001b[1mBitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\u001b[0m\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "/usr/local/venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BaichuanForSequenceClassification were not initialized from the model checkpoint at /DATA/jupyter/share/LLM_NBS/Baichuan2-7B-Chat/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2024-03-01 17:53:44.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m98\u001b[0m - \u001b[1mmemory footprint of model: 5.059646621346474 GB\u001b[0m\n",
      "\u001b[32m2024-03-01 17:53:44.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mload_model\u001b[0m:\u001b[36m103\u001b[0m - \u001b[1mTotal model params: 3786.69M\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 33,562,626 || all params: 7,024,693,252 || trainable%: 0.4777806630979137\n"
     ]
    }
   ],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.model_name_or_path = '/DATA/jupyter/share/LLM_NBS/Baichuan2-7B-Chat/'\n",
    "        self.fp16 = True\n",
    "        self.train_mode = 'qlora'\n",
    "        self.lora_rank=64\n",
    "        self.lora_dropout=0.05\n",
    "        self.lora_alpha = 16\n",
    "        self.gradient_checkpointing = False\n",
    "        self.task_type='SEQ_CLS'\n",
    "template = dict(\n",
    "    template_name='baichuan2',\n",
    "    system_format=None,\n",
    "    user_format='<reserved_106>{content}<reserved_107>',\n",
    "    assistant_format='{content}</s>',\n",
    "    system=None,\n",
    "    stop_word='</s>'\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "args = Arguments()\n",
    "tokenizer = load_tokenizer(args)\n",
    "d = load_model(args)\n",
    "model = d['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c00da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['你是谁', '这个多少钱', 'what is wrong with u']\n",
    "labels = [0, 1, 0]\n",
    "labels = torch.tensor(labels).long()\n",
    "message = template['user_format'].format(content = text)\n",
    "encode = tokenizer.batch_encode_plus(text, return_tensors='pt', padding='longest', max_length=1024)\n",
    "encode['prompt_lengths'] = (encode.input_ids != tokenizer.pad_token_id).sum(axis=1)\n",
    "encode['labels'] = labels\n",
    "for k, v in encode.items():\n",
    "    encode[k] = v.to(model.device)\n",
    "encode2 = dict((k, v) for k, v in encode.items() if k != 'prompt_lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba363d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8710370063781738, True\n",
      "SequenceClassifierOutput(loss=tensor(1.8710, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2901,  0.7803],\n",
      "        [ 1.7586, -2.3720],\n",
      "        [-0.5060, -2.7436]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(1.8710, device='cuda:0', grad_fn=<NllLossBackward0>), logits=tensor([[-0.2901,  0.7803],\n",
       "        [ 1.7586, -2.3720],\n",
       "        [-0.5060, -2.7436]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model(return_dict=True, **encode)\n",
    "o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv-kernel",
   "language": "python",
   "name": "local-venv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
